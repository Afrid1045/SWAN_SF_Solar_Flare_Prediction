{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "from pyts.classification import TimeSeriesForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pywt\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Partition 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"/home/afrid/Documents/NASA/dataverse_files/partition1\"\n",
    "folder_name_1 = 'FL'\n",
    "folder_name_2 = 'NF'\n",
    "folder_path_1 = os.path.join(main_folder_path, folder_name_1)\n",
    "folder_path_2 = os.path.join(main_folder_path, folder_name_2)\n",
    "files_folder_1 = os.listdir(folder_path_1)\n",
    "files_folder_2 = os.listdir(folder_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_folder_1 = [pd.read_csv(os.path.join(folder_path_1, file), sep='\\t', usecols=[1, 2, 3]).assign(Class=1) for file in files_folder_1]\n",
    "dfs_folder_2 = [pd.read_csv(os.path.join(folder_path_2, file), sep='\\t', usecols=[1, 2, 3]).assign(Class=0) for file in files_folder_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after loading CSV files:\n",
      "TOTUSJH    float64\n",
      "TOTBSQ     float64\n",
      "TOTPOT     float64\n",
      "Class        int64\n",
      "dtype: object\n",
      "TOTUSJH    float64\n",
      "TOTBSQ     float64\n",
      "TOTPOT     float64\n",
      "Class        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_folder_1 = np.stack([df.values for df in dfs_folder_1], axis=0)\n",
    "data_folder_2 = np.stack([df.values for df in dfs_folder_2], axis=0)\n",
    "\n",
    "print(\"Data types after loading CSV files:\")\n",
    "print(dfs_folder_1[0].dtypes)\n",
    "print(dfs_folder_2[0].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after conversion:\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "data_folder_1[:, :, -1] = data_folder_1[:, :, -1].astype(int)\n",
    "data_folder_2[:, :, -1] = data_folder_2[:, :, -1].astype(int)\n",
    "\n",
    "\n",
    "print(\"Data types after conversion:\")\n",
    "print(data_folder_1[:, :, -1].dtype)\n",
    "print(data_folder_2[:, :, -1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = np.concatenate([data_folder_1, data_folder_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values present in the 5th subarray of data_folder_1.\n"
     ]
    }
   ],
   "source": [
    "fifth_subarray = data_folder_1[4]\n",
    "\n",
    "missing_values_present = np.isnan(fifth_subarray).any()\n",
    "\n",
    "if missing_values_present:\n",
    "    print(\"Missing values present in the 5th subarray of data_folder_1.\")\n",
    "else:\n",
    "    print(\"No missing values in the 5th subarray of data_folder_1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarrays_with_missing_values = []\n",
    "for i, subarray in enumerate(final_data):\n",
    "    missing_values_present = np.isnan(subarray).any()\n",
    "    if missing_values_present:\n",
    "        subarrays_with_missing_values.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarrays_with_missing_values = []\n",
    "for i, subarray in enumerate(final_data):\n",
    "    subarray = np.array(subarray)\n",
    "    missing_values_present = np.isnan(subarray)\n",
    "    if np.any(missing_values_present):\n",
    "        subarrays_with_missing_values.append(i)\n",
    "        column_means = np.nanmean(subarray, axis=0)\n",
    "        for j in range(subarray.shape[1]):\n",
    "            subarray[:, j][missing_values_present[:, j]] = column_means[j]\n",
    "        final_data[i] = subarray.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_per_array = [\n",
    "    np.any(np.isnan(array)) for array in final_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1254, 60, 4)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72238, 60, 4)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73492, 60, 4)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.31512064e+03, 1.84249705e+10, 3.01690142e+23, 1.00000000e+00],\n",
       "        [1.29529138e+03, 1.84362189e+10, 3.02408756e+23, 1.00000000e+00],\n",
       "        [1.29107235e+03, 1.84317213e+10, 3.01902259e+23, 1.00000000e+00],\n",
       "        ...,\n",
       "        [1.26301717e+03, 1.70505430e+10, 2.86850258e+23, 1.00000000e+00],\n",
       "        [1.25975811e+03, 1.70497188e+10, 2.87663620e+23, 1.00000000e+00],\n",
       "        [1.26777601e+03, 1.70489327e+10, 2.88869871e+23, 1.00000000e+00]],\n",
       "\n",
       "       [[1.69928306e+03, 1.89409482e+10, 3.39950428e+23, 1.00000000e+00],\n",
       "        [1.70093419e+03, 1.88158286e+10, 3.39971390e+23, 1.00000000e+00],\n",
       "        [1.68612987e+03, 1.87045488e+10, 3.39017681e+23, 1.00000000e+00],\n",
       "        ...,\n",
       "        [1.75297986e+03, 1.91046967e+10, 3.47880711e+23, 1.00000000e+00],\n",
       "        [1.77367458e+03, 1.93368172e+10, 3.52739906e+23, 1.00000000e+00],\n",
       "        [1.83538591e+03, 1.96835259e+10, 3.58910389e+23, 1.00000000e+00]],\n",
       "\n",
       "       [[4.09370433e+03, 6.38030309e+10, 1.06127205e+24, 1.00000000e+00],\n",
       "        [4.07213919e+03, 6.37715638e+10, 1.06753144e+24, 1.00000000e+00],\n",
       "        [4.03976200e+03, 6.35553936e+10, 1.05659416e+24, 1.00000000e+00],\n",
       "        ...,\n",
       "        [4.22887736e+03, 6.42276961e+10, 1.03726723e+24, 1.00000000e+00],\n",
       "        [4.19831435e+03, 6.38042043e+10, 1.03053802e+24, 1.00000000e+00],\n",
       "        [4.19941425e+03, 6.33760272e+10, 1.01846623e+24, 1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[4.62710656e+02, 3.74540874e+09, 3.76829099e+22, 0.00000000e+00],\n",
       "        [4.64801310e+02, 3.78869083e+09, 3.78516770e+22, 0.00000000e+00],\n",
       "        [4.52926291e+02, 3.73455327e+09, 3.74736871e+22, 0.00000000e+00],\n",
       "        ...,\n",
       "        [3.53181051e+02, 2.90660715e+09, 2.62081445e+22, 0.00000000e+00],\n",
       "        [3.40214590e+02, 2.82870622e+09, 2.54906475e+22, 0.00000000e+00],\n",
       "        [3.28827661e+02, 2.76154609e+09, 2.50222833e+22, 0.00000000e+00]],\n",
       "\n",
       "       [[2.82483755e+01, 1.93816550e+08, 1.26635144e+21, 0.00000000e+00],\n",
       "        [3.02140837e+01, 1.97330403e+08, 1.29268561e+21, 0.00000000e+00],\n",
       "        [3.06673127e+01, 1.96906043e+08, 1.31932992e+21, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.43117728e+01, 1.37904602e+08, 1.16439361e+21, 0.00000000e+00],\n",
       "        [2.17419983e+01, 1.42813216e+08, 1.19702241e+21, 0.00000000e+00],\n",
       "        [2.16129491e+01, 1.33384880e+08, 1.02754589e+21, 0.00000000e+00]],\n",
       "\n",
       "       [[7.73875476e+01, 6.10209892e+08, 4.72693160e+21, 0.00000000e+00],\n",
       "        [7.74615841e+01, 6.15162783e+08, 4.75834368e+21, 0.00000000e+00],\n",
       "        [7.71755030e+01, 6.12877005e+08, 4.81110765e+21, 0.00000000e+00],\n",
       "        ...,\n",
       "        [7.25080055e+01, 5.99644669e+08, 4.45360521e+21, 0.00000000e+00],\n",
       "        [7.26222937e+01, 6.00144504e+08, 4.38731779e+21, 0.00000000e+00],\n",
       "        [7.18636646e+01, 6.04342714e+08, 4.44605464e+21, 0.00000000e+00]]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 72238, 1.0: 1254}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(final_data[:, 0, -1], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each class: {0.0: 72238, 1.0: 1254}\n",
      "Indexes for class 0.0: [ 1254  1255  1256 ... 73489 73490 73491]\n",
      "Indexes for class 1.0: [   0    1    2 ... 1251 1252 1253]\n"
     ]
    }
   ],
   "source": [
    "unique_labels, counts = np.unique(final_data[:, 0, -1], return_counts=True)\n",
    "label_counts = dict(zip(unique_labels, counts))\n",
    "print(\"Counts for each class:\", label_counts)\n",
    "\n",
    "\n",
    "indexes = {}\n",
    "for label in unique_labels:\n",
    "    indexes[label] = np.where(final_data[:, 0, -1] == label)[0]\n",
    "    print(f\"Indexes for class {label}: {indexes[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Partition 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path_p2 = \"/home/afrid/Documents/NASA/dataverse_files/partition2\"\n",
    "folder_name_FL_p2 = 'FL'\n",
    "folder_name_NF_p2 = 'NF'\n",
    "\n",
    "folder_path_FL_p2 = os.path.join(main_folder_path_p2, folder_name_FL_p2)\n",
    "folder_path_NF_p2 = os.path.join(main_folder_path_p2, folder_name_NF_p2)\n",
    "files_FL_p2 = os.listdir(folder_path_FL_p2)\n",
    "files_NF_p2 = os.listdir(folder_path_NF_p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_FL_p2 = [pd.read_csv(os.path.join(folder_path_FL_p2, file), sep='\\t', usecols=[1, 2, 3]).assign(Class=1) for file in files_FL_p2]\n",
    "dfs_NF_p2 = [pd.read_csv(os.path.join(folder_path_NF_p2, file), sep='\\t', usecols=[1, 2, 3]).assign(Class=0) for file in files_NF_p2]\n",
    "data_FL_p2 = np.stack([df.values for df in dfs_FL_p2], axis=0)\n",
    "data_NF_p2 = np.stack([df.values for df in dfs_NF_p2], axis=0)\n",
    "data_FL_p2[:, :, -1] = data_FL_p2[:, :, -1].astype(int)\n",
    "data_NF_p2[:, :, -1] = data_NF_p2[:, :, -1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_p2 = np.concatenate([data_FL_p2, data_NF_p2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarrays_with_missing_values_NF_p2 = []\n",
    "\n",
    "for i, subarray in enumerate(final_data_p2):\n",
    "    subarray_NF_p2 = np.array(subarray)\n",
    "    missing_values_present_NF_p2 = np.isnan(subarray_NF_p2)\n",
    "\n",
    "    if np.any(missing_values_present_NF_p2):\n",
    "        subarrays_with_missing_values_NF_p2.append(i)\n",
    "        column_means_NF_p2 = np.nanmean(subarray_NF_p2, axis=0)\n",
    "        for j_NF_p2 in range(subarray_NF_p2.shape[1]):\n",
    "            subarray_NF_p2[:, j_NF_p2][missing_values_present_NF_p2[:, j_NF_p2]] = column_means_NF_p2[j_NF_p2]\n",
    "        final_data_p2[i] = subarray_NF_p2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_data(X, y):\n",
    "    dataset = np.hstack((X.reshape(X.shape[0], -1), y.reshape(-1, 1)))\n",
    "    majority = dataset[dataset[:, -1] == 0.0]\n",
    "    minority = dataset[dataset[:, -1] == 1.0]\n",
    "    majority_undersampled = resample(majority, replace=False, n_samples=len(minority), random_state=123)\n",
    "    dataset_undersampled = np.vstack((minority, majority_undersampled))\n",
    "    X_undersampled = dataset_undersampled[:, :-1].reshape(-1, X.shape[1], X.shape[2])\n",
    "    y_undersampled = dataset_undersampled[:, -1]\n",
    "\n",
    "    return X_undersampled, y_undersampled\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_normalize_3d(data):\n",
    "    scaled_subarrays = []\n",
    "    for subarray in data:\n",
    "        scaler = RobustScaler()\n",
    "        scaled_subarray = scaler.fit_transform(subarray[:, :-1])\n",
    "        scaled_subarray_with_label = np.hstack((scaled_subarray, subarray[:, -1].reshape(-1, 1)))\n",
    "        scaled_subarrays.append(scaled_subarray_with_label)\n",
    "\n",
    "    scaled_data = np.array(scaled_subarrays)\n",
    "    return scaled_data\n",
    "\n",
    "#normalized_train_data = robust_normalize_3d(final_train_data)\n",
    "\n",
    "#normalized_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wavelet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dwt(data, wavelet='db4', level=3):\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=0)\n",
    "    concatenated = np.concatenate(coeffs, axis=0)\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSS and HSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scorer(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if cm.shape == (2, 2):\n",
    "        tp = cm[1, 1]\n",
    "        fn = cm[1, 0]\n",
    "        fp = cm[0, 1]\n",
    "        tn = cm[0, 0]\n",
    "\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "        tss = sensitivity - fpr\n",
    "\n",
    "        p = tp + fn\n",
    "        n = fp + tn\n",
    "        hss = (2 * ((tp * tn) - (fn * fp))) / ((p * (fn + tn)) + (n * (tp + fp)))\n",
    "\n",
    "        return tss, hss\n",
    "    elif cm.shape == (1, 1):\n",
    "        return 1 if y_true[0] == y_pred[0] else 0, 0\n",
    "    else:\n",
    "        print(f\"Error: Confusion matrix shape {cm.shape} is not handled.\")\n",
    "        return 0, 0\n",
    "\n",
    "tss_scorer = make_scorer(lambda y_true, y_pred: custom_scorer(y_true, y_pred)[0], greater_is_better=True)\n",
    "hss_scorer = make_scorer(lambda y_true, y_pred: custom_scorer(y_true, y_pred)[1], greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultivariateClassifier(estimator=TimeSeriesForest(random_state=53))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    tss, hss = custom_scorer(y_test, y_pred)\n",
    "    return tss, hss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7457747889252184, 0.10261476813672603)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Undersampled data (on only training data) + Wavelet Transform\n",
    "X_train_under, y_train_under = undersample_data(final_data[:, :, :-1], final_data[:, 0, -1])\n",
    "X_train_under_wave = np.array([apply_dwt(subarray, 'db4', 3) for subarray in X_train_under])\n",
    "X_test_wave = np.array([apply_dwt(subarray[:, :-1], 'db4', 3) for subarray in final_data_p2])\n",
    "results['scenario_1'] = evaluate_model(X_train_under_wave, y_train_under, X_test_wave, final_data_p2[:, 0, -1], clf)\n",
    "results['scenario_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7489300998573467, 0.7489300998573466)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Undersampled data (both on training and testing data) + wavelet transform\n",
    "X_test_under, y_test_under = undersample_data(final_data_p2[:, :, :-1], final_data_p2[:, 0, -1])\n",
    "X_test_under_wave = np.array([apply_dwt(subarray, 'db4', 3) for subarray in X_test_under])\n",
    "results['scenario_2'] = evaluate_model(X_train_under_wave, y_train_under, X_test_under_wave, y_test_under, clf)\n",
    "results['scenario_2'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7457747889252184, 0.10261476813672603)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Undersampled data (on only training data) + normalization + Wavelet Transform\n",
    "X_train_under_norm = robust_normalize_3d(X_train_under)\n",
    "X_train_under_norm_wave = np.array([apply_dwt(subarray, 'db4', 3) for subarray in X_train_under_norm])\n",
    "results['scenario_3'] = evaluate_model(X_train_under_norm_wave, y_train_under, X_test_wave, final_data_p2[:, 0, -1], clf)\n",
    "results['scenario_3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7489300998573467, 0.7489300998573466)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Undersampled data (both on training and testing data) + normalization + wavelet transform\n",
    "X_test_under_norm = robust_normalize_3d(X_test_under)\n",
    "X_test_under_norm_wave = np.array([apply_dwt(subarray, 'db4', 3) for subarray in X_test_under_norm])\n",
    "results['scenario_4'] = evaluate_model(X_train_under_norm_wave, y_train_under, X_test_under_norm_wave, y_test_under, clf)\n",
    "results['scenario_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario_1: TSS = 0.7457747889252184, HSS = 0.10261476813672603\n",
      "scenario_2: TSS = 0.7489300998573467, HSS = 0.7489300998573466\n",
      "scenario_3: TSS = 0.7457747889252184, HSS = 0.10261476813672603\n",
      "scenario_4: TSS = 0.7489300998573467, HSS = 0.7489300998573466\n"
     ]
    }
   ],
   "source": [
    "for scenario, (tss, hss) in results.items():\n",
    "    print(f\"{scenario}: TSS = {tss}, HSS = {hss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2508, 60, 4)\n"
     ]
    }
   ],
   "source": [
    "class_labels = final_data[:, 0, -1]\n",
    "class_0_indices = np.where(class_labels == 0)[0]\n",
    "class_1_indices = np.where(class_labels == 1)[0]\n",
    "selected_class_0_indices = class_0_indices[:len(class_1_indices)]\n",
    "selected_indices = np.concatenate([class_1_indices, selected_class_0_indices])\n",
    "final_train_data = final_data[selected_indices]\n",
    "final_train_data[:, 0, -1] = final_train_data[:, 0, -1].astype(int)\n",
    "\n",
    "print(final_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partition 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2804, 60, 4)\n"
     ]
    }
   ],
   "source": [
    "class_labels_p2 = final_data_p2[:, 0, -1]\n",
    "class_0_indices_p2 = np.where(class_labels_p2 == 0)[0]\n",
    "class_1_indices_p2 = np.where(class_labels_p2 == 1)[0]\n",
    "selected_class_0_indices_p2 = class_0_indices_p2[:len(class_1_indices_p2)]\n",
    "selected_indices_p2 = np.concatenate([class_1_indices_p2, selected_class_0_indices_p2])\n",
    "final_data_p2 = final_data_p2[selected_indices_p2]\n",
    "final_data_p2[:, 0, -1] = final_data_p2[:, 0, -1].astype(int)\n",
    "\n",
    "\n",
    "print(final_data_p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2508, 60, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.31512064e+03, 1.84249705e+10, 3.01690142e+23, 1.00000000e+00],\n",
       "        [1.29529138e+03, 1.84362189e+10, 3.02408756e+23, 1.00000000e+00],\n",
       "        [1.29107235e+03, 1.84317213e+10, 3.01902259e+23, 1.00000000e+00],\n",
       "        ...,\n",
       "        [1.26301717e+03, 1.70505430e+10, 2.86850258e+23, 1.00000000e+00],\n",
       "        [1.25975811e+03, 1.70497188e+10, 2.87663620e+23, 1.00000000e+00],\n",
       "        [1.26777601e+03, 1.70489327e+10, 2.88869871e+23, 1.00000000e+00]],\n",
       "\n",
       "       [[1.69928306e+03, 1.89409482e+10, 3.39950428e+23, 1.00000000e+00],\n",
       "        [1.70093419e+03, 1.88158286e+10, 3.39971390e+23, 1.00000000e+00],\n",
       "        [1.68612987e+03, 1.87045488e+10, 3.39017681e+23, 1.00000000e+00],\n",
       "        ...,\n",
       "        [1.75297986e+03, 1.91046967e+10, 3.47880711e+23, 1.00000000e+00],\n",
       "        [1.77367458e+03, 1.93368172e+10, 3.52739906e+23, 1.00000000e+00],\n",
       "        [1.83538591e+03, 1.96835259e+10, 3.58910389e+23, 1.00000000e+00]],\n",
       "\n",
       "       [[4.09370433e+03, 6.38030309e+10, 1.06127205e+24, 1.00000000e+00],\n",
       "        [4.07213919e+03, 6.37715638e+10, 1.06753144e+24, 1.00000000e+00],\n",
       "        [4.03976200e+03, 6.35553936e+10, 1.05659416e+24, 1.00000000e+00],\n",
       "        ...,\n",
       "        [4.22887736e+03, 6.42276961e+10, 1.03726723e+24, 1.00000000e+00],\n",
       "        [4.19831435e+03, 6.38042043e+10, 1.03053802e+24, 1.00000000e+00],\n",
       "        [4.19941425e+03, 6.33760272e+10, 1.01846623e+24, 1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.45791728e+01, 1.77831095e+08, 1.30115168e+21, 0.00000000e+00],\n",
       "        [2.42094979e+01, 1.75703261e+08, 1.35057692e+21, 0.00000000e+00],\n",
       "        [2.78213917e+01, 1.86367413e+08, 1.39998998e+21, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.77683827e+01, 1.13977714e+08, 1.13855715e+21, 0.00000000e+00],\n",
       "        [1.77911230e+01, 1.17515906e+08, 1.22072647e+21, 0.00000000e+00],\n",
       "        [2.36156556e+01, 1.37117645e+08, 1.52585774e+21, 0.00000000e+00]],\n",
       "\n",
       "       [[8.93914920e+01, 1.08836211e+09, 4.15965666e+21, 0.00000000e+00],\n",
       "        [9.43815392e+01, 1.12551157e+09, 4.32095010e+21, 0.00000000e+00],\n",
       "        [9.63383318e+01, 1.11693837e+09, 4.38273334e+21, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.36919272e+02, 1.54799028e+09, 7.59350481e+21, 0.00000000e+00],\n",
       "        [1.35564834e+02, 1.56576383e+09, 7.76909276e+21, 0.00000000e+00],\n",
       "        [1.30022672e+02, 1.52874959e+09, 7.52277424e+21, 0.00000000e+00]],\n",
       "\n",
       "       [[3.43042407e+02, 2.19676136e+09, 1.91368173e+22, 0.00000000e+00],\n",
       "        [3.37655165e+02, 2.19257301e+09, 1.94131339e+22, 0.00000000e+00],\n",
       "        [3.49005313e+02, 2.20447872e+09, 1.96210393e+22, 0.00000000e+00],\n",
       "        ...,\n",
       "        [3.67732021e+02, 2.45234263e+09, 2.03979821e+22, 0.00000000e+00],\n",
       "        [3.67915081e+02, 2.43876773e+09, 2.02479422e+22, 0.00000000e+00],\n",
       "        [3.73983855e+02, 2.44105434e+09, 2.01548062e+22, 0.00000000e+00]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.10433562e+03, 1.66309802e+10, 2.44271097e+23, 1.00000000e+00],\n",
       "        [2.13771943e+03, 1.66821080e+10, 2.43289777e+23, 1.00000000e+00],\n",
       "        [2.13605186e+03, 1.67190788e+10, 2.43625786e+23, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.17481045e+03, 1.74546880e+10, 2.90306262e+23, 1.00000000e+00],\n",
       "        [2.14550720e+03, 1.74297430e+10, 2.90576637e+23, 1.00000000e+00],\n",
       "        [2.11443359e+03, 1.73691727e+10, 2.84922323e+23, 1.00000000e+00]],\n",
       "\n",
       "       [[4.46694782e+03, 6.97694787e+10, 9.63171118e+23, 1.00000000e+00],\n",
       "        [4.49201943e+03, 7.00970991e+10, 9.72063961e+23, 1.00000000e+00],\n",
       "        [4.56087991e+03, 7.02730264e+10, 9.72636252e+23, 1.00000000e+00],\n",
       "        ...,\n",
       "        [4.42512551e+03, 6.63204200e+10, 9.61936558e+23, 1.00000000e+00],\n",
       "        [4.40896833e+03, 6.62384349e+10, 9.59023923e+23, 1.00000000e+00],\n",
       "        [4.41834384e+03, 6.60643521e+10, 9.58056537e+23, 1.00000000e+00]],\n",
       "\n",
       "       [[2.31663333e+03, 2.96306831e+10, 3.42228002e+23, 1.00000000e+00],\n",
       "        [2.29150065e+03, 2.93650733e+10, 3.43373491e+23, 1.00000000e+00],\n",
       "        [2.29804453e+03, 2.92767008e+10, 3.37013102e+23, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.48246355e+03, 3.48070301e+10, 4.11115457e+23, 1.00000000e+00],\n",
       "        [2.46336095e+03, 3.47090386e+10, 4.01989781e+23, 1.00000000e+00],\n",
       "        [2.47559977e+03, 3.47276866e+10, 4.13016624e+23, 1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.47884712e+03, 2.42169520e+10, 3.82231260e+23, 0.00000000e+00],\n",
       "        [1.48427775e+03, 2.41148224e+10, 3.76839651e+23, 0.00000000e+00],\n",
       "        [1.46596949e+03, 2.41503853e+10, 3.83632048e+23, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.26388775e+03, 2.31216374e+10, 3.66427821e+23, 0.00000000e+00],\n",
       "        [1.24866046e+03, 2.31011418e+10, 3.69067224e+23, 0.00000000e+00],\n",
       "        [1.23567134e+03, 2.30486590e+10, 3.66987335e+23, 0.00000000e+00]],\n",
       "\n",
       "       [[1.21944319e+03, 1.92091963e+10, 2.21024836e+23, 0.00000000e+00],\n",
       "        [1.24054348e+03, 1.90749729e+10, 2.18588938e+23, 0.00000000e+00],\n",
       "        [1.23800279e+03, 1.89122221e+10, 2.17779630e+23, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.13194701e+03, 1.56615851e+10, 1.82362219e+23, 0.00000000e+00],\n",
       "        [1.14092382e+03, 1.56672271e+10, 1.83184582e+23, 0.00000000e+00],\n",
       "        [1.15782074e+03, 1.57190657e+10, 1.83450044e+23, 0.00000000e+00]],\n",
       "\n",
       "       [[6.67467289e+01, 5.15377045e+08, 4.01794062e+21, 0.00000000e+00],\n",
       "        [6.52739182e+01, 5.17023769e+08, 3.91720645e+21, 0.00000000e+00],\n",
       "        [6.58229314e+01, 5.14638573e+08, 4.05331553e+21, 0.00000000e+00],\n",
       "        ...,\n",
       "        [6.44524180e+01, 5.65277896e+08, 4.08526094e+21, 0.00000000e+00],\n",
       "        [6.46980618e+01, 5.60418968e+08, 4.01450084e+21, 0.00000000e+00],\n",
       "        [6.54771139e+01, 5.73460474e+08, 4.07955745e+21, 0.00000000e+00]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2508, 60, 4)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def robust_normalize_3d(data):\n",
    "    \"\"\"\n",
    "    Apply RobustScaler to each 2D subarray in a 3D array.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 3D numpy array, shape (n_samples, n_timepoints, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    - scaled_data: 3D numpy array with the same shape, where each 2D subarray has been scaled.\n",
    "    \"\"\"\n",
    " \n",
    "    scaled_subarrays = []\n",
    "    for subarray in data:\n",
    "        scaler = RobustScaler()\n",
    "        scaled_subarray = scaler.fit_transform(subarray[:, :-1])\n",
    "        scaled_subarray_with_label = np.hstack((scaled_subarray, subarray[:, -1].reshape(-1, 1)))\n",
    "        scaled_subarrays.append(scaled_subarray_with_label)\n",
    "\n",
    "    scaled_data = np.array(scaled_subarrays)\n",
    "    return scaled_data\n",
    "\n",
    "normalized_train_data = robust_normalize_3d(final_train_data)\n",
    "\n",
    "normalized_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_dwt(data, wavelet='db4', level=3):\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level, axis=0)\n",
    "    concatenated = np.concatenate(coeffs, axis=0)\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data = np.array([apply_dwt(subarray[:, :-1], 'db4', 3) for subarray in final_train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2508, 79, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_data = np.array([apply_dwt(subarray[:, :-1], 'db4', 3) for subarray in final_data_p2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultivariateClassifier(estimator=TimeSeriesForest(random_state=53))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = final_train_data[:, 0, -1]\n",
    "train_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each class: {0.0: 1254, 1.0: 1254}\n",
      "Indexes for class 0.0: [1254 1255 1256 ... 2505 2506 2507]\n",
      "Indexes for class 1.0: [   0    1    2 ... 1251 1252 1253]\n"
     ]
    }
   ],
   "source": [
    "unique_labels, counts = np.unique(train_labels, return_counts=True)\n",
    "label_counts = dict(zip(unique_labels, counts))\n",
    "print(\"Counts for each class:\", label_counts)\n",
    "\n",
    "\n",
    "indexes = {}\n",
    "for label in unique_labels:\n",
    "    indexes[label] = np.where(train_labels == label)[0]\n",
    "    print(f\"Indexes for class {label}: {indexes[label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultivariateClassifier(estimator=TimeSeriesForest(random_state=53))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultivariateClassifier</label><div class=\"sk-toggleable__content\"><pre>MultivariateClassifier(estimator=TimeSeriesForest(random_state=53))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: TimeSeriesForest</label><div class=\"sk-toggleable__content\"><pre>TimeSeriesForest(random_state=53)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TimeSeriesForest</label><div class=\"sk-toggleable__content\"><pre>TimeSeriesForest(random_state=53)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultivariateClassifier(estimator=TimeSeriesForest(random_state=53))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(transformed_train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = final_data_p2[:, 0, -1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each class: {0.0: 1402, 1.0: 1402}\n",
      "Indexes for class 0.0: [1402 1403 1404 ... 2801 2802 2803]\n",
      "Indexes for class 1.0: [   0    1    2 ... 1399 1400 1401]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_labels, counts = np.unique(test_labels, return_counts=True)\n",
    "label_counts = dict(zip(unique_labels, counts))\n",
    "print(\"Counts for each class:\", label_counts)\n",
    "\n",
    "indexes = {}\n",
    "for label in unique_labels:\n",
    "    indexes[label] = np.where(test_labels == label)[0]\n",
    "    print(f\"Indexes for class {label}: {indexes[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(transformed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scorer(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if cm.shape == (2, 2):\n",
    "        tp = cm[1, 1]\n",
    "        fn = cm[1, 0]\n",
    "        fp = cm[0, 1]\n",
    "        tn = cm[0, 0]\n",
    "\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "        tss = sensitivity - fpr\n",
    "\n",
    "        p = tp + fn\n",
    "        n = fp + tn\n",
    "        hss = (2 * ((tp * tn) - (fn * fp))) / ((p * (fn + tn)) + (n * (tp + fp)))\n",
    "\n",
    "        return tss, hss\n",
    "    elif cm.shape == (1, 1):\n",
    "        return 1 if y_true[0] == y_pred[0] else 0, 0\n",
    "    else:\n",
    "        print(f\"Error: Confusion matrix shape {cm.shape} is not handled.\")\n",
    "        return 0, 0\n",
    "\n",
    "tss_scorer = make_scorer(lambda y_true, y_pred: custom_scorer(y_true, y_pred)[0], greater_is_better=True)\n",
    "hss_scorer = make_scorer(lambda y_true, y_pred: custom_scorer(y_true, y_pred)[1], greater_is_better=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.7503566333808844\n",
      "HSS: 0.7503566333808844\n"
     ]
    }
   ],
   "source": [
    "tss, hss = custom_scorer(test_labels, predictions)\n",
    "\n",
    "print(f\"TSS: {tss}\")\n",
    "print(f\"HSS: {hss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
